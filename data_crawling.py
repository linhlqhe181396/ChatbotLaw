import requests
from bs4 import BeautifulSoup
import json
import re

# URL trang web c·∫ßn crawl
URL = "https://thuvienphapluat.vn/van-ban/Bat-dong-san/Thong-tu-11-2021-TT-BTNMT-Dinh-muc-kinh-te-ky-thuat-lap-quy-hoach-ke-hoach-su-dung-dat-483793.aspx"

# H√†m g·ª≠i request v√† l·∫•y HTML
def get_html(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.text
    else:
        print(f"Error {response.status_code}: Unable to fetch page")
        return None

# H√†m l√†m s·∫°ch vƒÉn b·∫£n: lo·∫°i b·ªè \r, \n, \t v√† c√°c kho·∫£ng tr·∫Øng th·ª´a, chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
def clean_text(text):
    text = re.sub(r'[\r\n\t]+', ' ', text)  # Lo·∫°i b·ªè \r, \n, \t thay b·∫±ng kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text).strip()  # X√≥a kho·∫£ng tr·∫Øng th·ª´a
    return text.lower()  # üîπ Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng

# H√†m ph√¢n t√≠ch n·ªôi dung VBPL v√† l·ªçc <p> theo v·ªã tr√≠
def parse_phapluat_page(html, start_index=0, end_index=None):
    soup = BeautifulSoup(html, "html.parser")

    # T√¨m ph·∫ßn n·ªôi dung ch√≠nh c·ªßa vƒÉn b·∫£n ph√°p lu·∫≠t
    content_div = soup.find("div", {"id": "ctl00_Content_ThongTinVB_divNoiDung"})
    if not content_div:
        print("Kh√¥ng t√¨m th·∫•y n·ªôi dung b√†i vi·∫øt")
        return None

    #  L·∫•y t·∫•t c·∫£ c√°c th·∫ª <p> trong ph·∫°m vi mong mu·ªën
    all_paragraphs = content_div.find_all("p")
    filtered_paragraphs = all_paragraphs[start_index:end_index]  # L·ªçc theo v·ªã tr√≠

    #  L∆∞u n·ªôi dung <p> v√†o danh s√°ch, lo·∫°i b·ªè k√Ω t·ª± kh√¥ng mong mu·ªën v√† chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    extracted_content = [clean_text(p.get_text()) for p in filtered_paragraphs if p.get_text(strip=True)]

    return extracted_content

# H√†m l∆∞u d·ªØ li·ªáu v√†o file JSON
def save_to_json(data, filename="TT47_1.json"):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

# Ch·∫°y qu√° tr√¨nh crawl v·ªõi l·ªçc v·ªã tr√≠ <p>
html_content = get_html(URL)
if html_content:
    structured_data = parse_phapluat_page(html_content, start_index=4, end_index=None)  #  ƒêi·ªÅu ch·ªânh ph·∫°m vi
    save_to_json(structured_data)
    
